{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05wV7JTKKcfR",
        "outputId": "3cc37f5f-7970-43ab-e44e-659bd692dcae"
      },
      "source": [
        "!git clone https://github.com/mostafachatillon/ChemGAN-challenge.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ChemGAN-challenge'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 1637 (delta 0), reused 0 (delta 0), pack-reused 1634\u001b[K\n",
            "Receiving objects: 100% (1637/1637), 244.51 MiB | 26.35 MiB/s, done.\n",
            "Resolving deltas: 100% (182/182), done.\n",
            "Checking out files: 100% (1718/1718), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqL9l7PXKs7g",
        "outputId": "30909632-314b-4558-dd38-c7b403de2af3"
      },
      "source": [
        "!pip install tensorflow==1.0.1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/7935eb82b9a9b89a3a8ef7e54f7d538698c85d248d8bedb533eab5afd293/tensorflow-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (44.5MB)\n",
            "\u001b[K     |████████████████████████████████| 44.5MB 98kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.1) (1.19.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.1) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.1) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.1) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.1.0->tensorflow==1.0.1) (50.3.2)\n",
            "Installing collected packages: tensorflow\n",
            "  Found existing installation: tensorflow 2.4.0\n",
            "    Uninstalling tensorflow-2.4.0:\n",
            "      Successfully uninstalled tensorflow-2.4.0\n",
            "Successfully installed tensorflow-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "749droeBK8Uo",
        "outputId": "b27ecbd0-eb86-4d6d-bc7f-99705758f9ab"
      },
      "source": [
        "url = \"https://anaconda.org/rdkit/rdkit/2017.03.3/download/linux-64/rdkit-2017.03.3-np111py27_1.tar.bz2\"\r\n",
        "!curl -L $url | tar xj lib"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  3765    0  3765    0     0   7699      0 --:--:-- --:--:-- --:--:--  7683\n",
            "100 18.8M  100 18.8M    0     0  5252k      0  0:00:03  0:00:03 --:--:-- 7747k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqTU5qUVLJ_p"
      },
      "source": [
        "!mv lib/*.so.* $x86/"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAXxKSEbLapE"
      },
      "source": [
        "!ln -s $x86/libboost_python3-py36.so.1.65.1 $x86/libboost_python3.so.1.65.1\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQlEAPngLemE",
        "outputId": "7016ddc9-ed35-4ebb-af02-8f6f6c5f10ec"
      },
      "source": [
        "!python train_ogan.py exp.json"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:455: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:456: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:457: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "mol_metrics: reading NP model ...\n",
            "loaded in 0.1293482780456543\n",
            "mol_metrics: reading SA model ...\n",
            "loaded in 0.5314326286315918\n",
            "Starting ObjectiveGAN for textCNN\n",
            "Data points in train_file   15001\n",
            "Max data length is            106\n",
            "Max length to use is           40\n",
            "Avg length to use is      33.980427\n",
            "Num valid data points is     4496\n",
            "Size of alphabet is            34\n",
            "Using parameters:\n",
            "EXP_NAME             - textCNN     \n",
            "TRAIN_FILE           - ../data/drugs_train_new.smi\n",
            "METRICS_FILE         - mol_metrics \n",
            "OBJECTIVE            - solubility  \n",
            "D_WEIGHT             -          0.5\n",
            "CHK_PATH             - checkpoints \n",
            "G_STEPS              -            1\n",
            "SEED                 -           88\n",
            "G_PRETRAIN_STEPS     -          240\n",
            "D_PRETRAIN_STEPS     -           50\n",
            "TOTAL_BATCH          -            2\n",
            "MAX_LENGTH           -           40\n",
            "BATCH_SIZE           -           64\n",
            "LOAD_PRETRAIN        -            1\n",
            "LOAD_PREV_SESS       -            1\n",
            "EPOCH_SAVES          -           20\n",
            "rest of parameters are set as default\n",
            "\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX512F instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
            "\t* No pre-training data found as checkpoints/textCNN_pretrain/pretrain_ckpt.\n",
            "Start pre-training...\n",
            "  0% 0/240 [00:00<?, ?it/s] gen pre-train\n",
            "\t test_loss 5.347479343414307, train_loss 2.8645474910736084\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "~~~ Summary Results ~~~\n",
            "Total samples   :   6400\n",
            "Unique          :   6400 (100.00%)\n",
            "Unverified      :   6400 (100.00%)\n",
            "Verified        :      0 (0.00%)\n",
            "\tmetrics...\n",
            "novelty              : nan\n",
            "hard_novelty         : nan\n",
            "soft_novelty         : nan\n",
            "diversity            : nan\n",
            "conciseness          : nan\n",
            "solubility           : nan\n",
            "naturalness          : nan\n",
            "synthesizability     : nan\n",
            "\n",
            "no good samples found :(\n",
            "\n",
            "Example of bad samples:\n",
            "\\IHO1c2(C_NC_C1cC_C_c_cC(cC2Hcc()3cN__nc\n",
            "H)))12)C_C_3Cc_nncCCc1cnc[]2_C)2)O]C__2)\n",
            ")[)cc_H_(__(_)c1(CO_1H_+cc(=nC__(c#cC2cC\n",
            "oc[n_ON)cl]cc_cn_(1c_(c@CcccC()COcrCC()1\n",
            "CSF_C11c3c1Fc)/NCcc)__1_C=2c__Ccc_N_Oc2c\n",
            ")]C32H1[-=NC(Nncc(2H_N_@N_CCc(c=1_C_n__(\n",
            "^cc-CCnc22C1_c1_C3c)cC)(c(ocC(_F)1cC2)C1\n",
            "5[5C)cC13Cc=C()C@cCCCnCOcCHSOn1_()_1)C1\n",
            "(_))(c)())cc2[c__)Cc)c=Cn@c)[CnC)H]Cc__)\n",
            "\\2=H(_)cCc_Cncn_(c_)c1c_cccC=c)s_C@1Nc(\n",
            "~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  0% 1/240 [00:07<28:04,  7.05s/it] gen pre-train\n",
            "  1% 2/240 [00:09<22:47,  5.75s/it] gen pre-train\n",
            "  1% 3/240 [00:12<19:11,  4.86s/it] gen pre-train\n",
            "  2% 4/240 [00:15<16:38,  4.23s/it] gen pre-train\n",
            "  2% 5/240 [00:18<14:47,  3.78s/it] gen pre-train\n",
            "  2% 6/240 [00:20<13:26,  3.45s/it] gen pre-train\n",
            "  3% 7/240 [00:23<12:31,  3.22s/it] gen pre-train\n",
            "  3% 8/240 [00:26<11:50,  3.06s/it] gen pre-train\n",
            "  4% 9/240 [00:28<11:20,  2.94s/it] gen pre-train\n",
            "  4% 10/240 [00:31<10:59,  2.87s/it] gen pre-train\n",
            "\t test_loss 5.318163871765137, train_loss 1.014971137046814\n",
            "~~~ Summary Results ~~~\n",
            "Total samples   :   6400\n",
            "Unique          :   6375 (99.61%)\n",
            "Unverified      :   6234 (97.41%)\n",
            "Verified        :    166 (2.59%)\n",
            "\tmetrics...\n",
            "novelty              : 1.0000\n",
            "hard_novelty         : 1.0000\n",
            "soft_novelty         : 1.0000\n",
            "diversity            : 0.6342\n",
            "conciseness          : 0.9825\n",
            "solubility           : 0.4615\n",
            "naturalness          : 0.6911\n",
            "synthesizability     : 0.5265\n",
            "\n",
            "Example of good samples:\n",
            "CC(=O)N(C(F)=O)NCC[C@@H]1CCC1\n",
            "CC(CCO)[C@H](C)c1cccc(O)c1C\n",
            "C1cncc1\n",
            "CCOCNC(=O)CC(=O)c1ccccc1OC\n",
            "CCc1ccc(OC(=O)NC(O)C)o1\n",
            "CO\n",
            "CC(=O)Nc1[n](C)c1\n",
            "CCc1nccn1C\n",
            "CSCCC(C)COC(N)(O)[C@H]2C(=O)NC(C)COCCSC2\n",
            "ON1CCN1C[C@H](C)C[NH+]1CC(=O)NC(C=O)CC1\n",
            "\n",
            "Example of bad samples:\n",
            "C(CSCC(=O)=C)C1CNc1ccccc2CN[NH+]1-=CC@P1\n",
            "O=cc(CCCc1nc(-c(F)cc3)CCOc1c1\n",
            "CCN(C)(C(F)CCC2)CC[O-](=O)CCCs1[CH]c2ccc\n",
            "CC(C(C)Cc2cccc3[S-])c2l)oc1F\n",
            "C[C@H](C(C)C)C(=O)CCC1\n",
            "C(N(C)CC=C1[NH3+]CCc2cccc(C)cc1)s1\n",
            "C[C@H]2C(=O)[C@H]2C(N)c2cccc2-c2F)cC1\n",
            "CC[c1c1cccc(=O\n",
            "O=C(OC(N)CC3c2cccc(C)c1\n",
            "CC(C)c1cncc(Nc1n21\n",
            "~~~~~~~~~~~~~~~~~~~~~~~\n",
            "  5% 11/240 [00:39<16:30,  4.33s/it] gen pre-train\n",
            "  5% 12/240 [00:41<14:34,  3.84s/it] gen pre-train\n",
            "  5% 13/240 [00:44<13:14,  3.50s/it] gen pre-train\n",
            "  6% 14/240 [00:47<12:17,  3.26s/it] gen pre-train\n",
            "  6% 15/240 [00:50<11:36,  3.10s/it] gen pre-train\n",
            "  7% 16/240 [00:52<11:06,  2.98s/it] gen pre-train\n",
            "  7% 17/240 [00:55<10:49,  2.91s/it] gen pre-train\n",
            "  8% 18/240 [00:58<10:30,  2.84s/it] gen pre-train\n",
            "  8% 19/240 [01:00<10:19,  2.80s/it] gen pre-train\n",
            "  8% 20/240 [01:03<10:09,  2.77s/it] gen pre-train\n",
            "  9% 21/240 [01:06<10:00,  2.74s/it] gen pre-train\n",
            "  9% 22/240 [01:08<09:54,  2.72s/it] gen pre-train\n",
            " 10% 23/240 [01:11<09:54,  2.74s/it] gen pre-train\n",
            " 10% 24/240 [01:14<09:49,  2.73s/it] gen pre-train\n",
            " 10% 25/240 [01:17<09:42,  2.71s/it] gen pre-train\n",
            " 11% 26/240 [01:19<09:38,  2.70s/it] gen pre-train\n",
            " 11% 27/240 [01:22<09:35,  2.70s/it] gen pre-train\n",
            " 12% 28/240 [01:25<09:32,  2.70s/it] gen pre-train\n",
            " 12% 29/240 [01:27<09:28,  2.70s/it] gen pre-train\n",
            " 12% 30/240 [01:30<09:25,  2.69s/it] gen pre-train\n",
            " 13% 31/240 [01:33<09:27,  2.71s/it] gen pre-train\n",
            " 13% 32/240 [01:35<09:22,  2.70s/it] gen pre-train\n",
            " 14% 33/240 [01:38<09:19,  2.70s/it] gen pre-train\n",
            " 14% 34/240 [01:41<09:16,  2.70s/it] gen pre-train\n",
            " 15% 35/240 [01:44<09:15,  2.71s/it] gen pre-train\n",
            " 15% 36/240 [01:46<09:14,  2.72s/it] gen pre-train\n",
            " 15% 37/240 [01:49<09:17,  2.75s/it] gen pre-train\n",
            " 16% 38/240 [01:52<09:12,  2.74s/it] gen pre-train\n",
            " 16% 39/240 [01:55<09:10,  2.74s/it] gen pre-train\n",
            " 17% 40/240 [01:57<09:08,  2.74s/it] gen pre-train\n",
            "\t test_loss 5.3105788230896, train_loss 0.8057687282562256\n",
            "~~~ Summary Results ~~~\n",
            "Total samples   :   6400\n",
            "Unique          :   6397 (99.95%)\n",
            "Unverified      :   5564 (86.94%)\n",
            "Verified        :    836 (13.06%)\n",
            "\tmetrics...\n",
            "novelty              : 1.0000\n",
            "hard_novelty         : 1.0000\n",
            "soft_novelty         : 1.0000\n",
            "diversity            : 0.4760\n",
            "conciseness          : 0.9781\n",
            "solubility           : 0.4914\n",
            "naturalness          : 0.5651\n",
            "synthesizability     : 0.5068\n",
            "\n",
            "Example of good samples:\n",
            "CN(CC[NH+](C)C)c1cnncc1\n",
            "CN1c2cc(F)ccc2C[NH+]1CCN1CCC1\n",
            "Cc1cc(N)c(Cl)cc1Cl\n",
            "COc1ccnnc1CNCC(=O)NCc1ccccc1OC\n",
            "Cc1cc(OCNC(=O)C)nn1CCNc1ccccn1\n",
            "CN1CCCN1CCN(C)c2ccccc2[C@@H]1CCCOCC1\n",
            "COc1cccc(-c2cc(C)ccc2C)c1\n",
            "Cc1c(OC(=O)C)noc1/CCOc1ccc(F)cc1Br\n",
            "CCN(C)c1ccccc1C(C)CCO\n",
            "CCN(Cc1ccccc1)N1CCCOC1\n",
            "\n",
            "Example of bad samples:\n",
            "CCC[S@@](CC=O)Cc1nnc(O)c1Br\n",
            "CC(C)Nc1ccc(N2CC[NH+](CC)C(N)=O)cc1Cl\n",
            "O=C(NCc1ccc(Cl)cc2N1\n",
            "N(Cc1ccccc21)c1ccc(Cl)c(C)c1C\n",
            "Cc1ccc(C(=O)NCS(=O)NCc1ncc(-c3nc(Cl)ncc3\n",
            "CC(=O)NC[C@@H]1CN1CCN1c2ccc(C(C)C)o2\n",
            "N(CC(N)=O)CS(=O)(=O)NC1\n",
            "N(C[NH2+]C[C@@H](O)Cc1ccccc1)C1\n",
            "CC(C)[C@@H]1[NH+](Cc1ccsc1\n",
            "CCc1nc(CC(=O)Ncc3nnc(C)nc2)cc1\n",
            "~~~~~~~~~~~~~~~~~~~~~~~\n",
            " 17% 41/240 [02:08<17:00,  5.13s/it] gen pre-train\n",
            " 18% 42/240 [02:11<14:34,  4.41s/it] gen pre-train\n",
            " 18% 43/240 [02:14<12:50,  3.91s/it] gen pre-train\n",
            " 18% 44/240 [02:16<11:35,  3.55s/it] gen pre-train\n",
            " 19% 45/240 [02:19<10:42,  3.29s/it] gen pre-train\n",
            " 19% 46/240 [02:22<10:04,  3.12s/it] gen pre-train\n",
            " 20% 47/240 [02:24<09:38,  3.00s/it] gen pre-train\n",
            " 20% 48/240 [02:27<09:19,  2.92s/it] gen pre-train\n",
            " 20% 49/240 [02:30<09:06,  2.86s/it] gen pre-train\n",
            " 21% 50/240 [02:33<08:58,  2.83s/it] gen pre-train\n",
            " 21% 51/240 [02:35<08:53,  2.82s/it] gen pre-train\n",
            " 22% 52/240 [02:38<08:47,  2.81s/it] gen pre-train\n",
            " 22% 53/240 [02:41<08:41,  2.79s/it] gen pre-train\n",
            " 22% 54/240 [02:44<08:36,  2.78s/it] gen pre-train\n",
            " 23% 55/240 [02:46<08:32,  2.77s/it] gen pre-train\n",
            " 23% 56/240 [02:49<08:29,  2.77s/it] gen pre-train\n",
            " 24% 57/240 [02:52<08:26,  2.77s/it] gen pre-train\n",
            " 24% 58/240 [02:55<08:22,  2.76s/it] gen pre-train\n",
            " 25% 59/240 [02:57<08:18,  2.75s/it] gen pre-train\n",
            " 25% 60/240 [03:00<08:16,  2.76s/it] gen pre-train\n",
            " 25% 61/240 [03:03<08:09,  2.74s/it] gen pre-train\n",
            " 26% 62/240 [03:06<08:04,  2.72s/it] gen pre-train\n",
            " 26% 63/240 [03:08<08:02,  2.72s/it] gen pre-train\n",
            " 27% 64/240 [03:11<07:58,  2.72s/it] gen pre-train\n",
            " 27% 65/240 [03:14<07:57,  2.73s/it] gen pre-train\n",
            " 28% 66/240 [03:16<07:53,  2.72s/it] gen pre-train\n",
            " 28% 67/240 [03:19<07:50,  2.72s/it] gen pre-train\n",
            " 28% 68/240 [03:22<07:49,  2.73s/it] gen pre-train\n",
            " 29% 69/240 [03:25<07:46,  2.73s/it] gen pre-train\n",
            " 29% 70/240 [03:27<07:42,  2.72s/it] gen pre-train\n",
            " 30% 71/240 [03:30<07:39,  2.72s/it] gen pre-train\n",
            " 30% 72/240 [03:33<07:34,  2.70s/it] gen pre-train\n",
            " 30% 73/240 [03:35<07:32,  2.71s/it] gen pre-train\n",
            " 31% 74/240 [03:38<07:30,  2.71s/it] gen pre-train\n",
            " 31% 75/240 [03:41<07:28,  2.72s/it] gen pre-train\n",
            " 32% 76/240 [03:44<07:24,  2.71s/it] gen pre-train\n",
            " 32% 77/240 [03:46<07:21,  2.71s/it] gen pre-train\n",
            " 32% 78/240 [03:49<07:20,  2.72s/it] gen pre-train\n",
            " 33% 79/240 [03:52<07:20,  2.74s/it] gen pre-train\n",
            " 33% 80/240 [03:55<07:20,  2.75s/it] gen pre-train\n",
            "\t test_loss 5.305029392242432, train_loss 0.7468510866165161\n",
            "~~~ Summary Results ~~~\n",
            "Total samples   :   6400\n",
            "Unique          :   6394 (99.91%)\n",
            "Unverified      :   4913 (76.77%)\n",
            "Verified        :   1487 (23.23%)\n",
            "\tmetrics...\n",
            "novelty              : 1.0000\n",
            "hard_novelty         : 1.0000\n",
            "soft_novelty         : 1.0000\n",
            "diversity            : 0.4293\n",
            "conciseness          : 0.9811\n",
            "solubility           : 0.5087\n",
            "naturalness          : 0.5307\n",
            "synthesizability     : 0.5613\n",
            "\n",
            "Example of good samples:\n",
            "COc1cccc(NCCOCc2ccccc2)c1C\n",
            "C/C(=O)/C=N/Cc1nnnc2c1C(=O)OCC2\n",
            "CCCC(C)C(=O)Nc1cc(C)ncc1C\n",
            "CCS(=O)(=O)NC(=O)C[N+](F)(F)F\n",
            "[NH3+][C@@H](Br)c1c(Br)ncs1\n",
            "C[C@H]1CC[NH+](Cc2cccs2)CCCC1\n",
            "CS[C@H]([C@H]1CCCC1)c1cc(Cl)ccc1F\n",
            "CCC(C)CCOC(=O)CCNC(=O)[C@H]1CCCS1\n",
            "C[C@]1(C)CCN(C(=O)NCc2ccccc2O)c1\n",
            "CCc1cccc(Cl)c1NC(C)=C\\C[C@@H]1CC1\n",
            "\n",
            "Example of bad samples:\n",
            "Cn1cc(F)c(NC(=O)c2cccc4ccccc32)c1C\n",
            "O=C(CCn1ncc(NC(=O)CO)cc1)c1ccc(C)c(C)c1\n",
            "C[C@@]1(c3ccc(O)n(CC)c2)CCSCC#NC1CCOC1\n",
            "COc1cc[n+]c(Nc2ccccc2Cl)cc1\n",
            "C[C@@]1(Cc2ccc3cccc3ccccc33)s2)c1\n",
            "COc1ncc(C2(C)CCOc3ccccc3)cn1)c1cccc(Cl)c\n",
            "CNc1ncc(C(C)(C)(=O)OCC(F)(C)C)cc1F\n",
            "C[NH+]1C(=O)NCc1ccc(C(=O)N2CC3)n1\n",
            "O=C(NCCC=C/O)C1CCN(Cc2cc4occcc2)C3)c1\n",
            "O=C([O-])c1cc([C@H](C)c2cccc(C)c2C)CCO1)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~\n",
            " 34% 81/240 [04:08<15:55,  6.01s/it] gen pre-train\n",
            " 34% 82/240 [04:11<13:13,  5.02s/it] gen pre-train\n",
            " 35% 83/240 [04:14<11:21,  4.34s/it] gen pre-train\n",
            " 35% 84/240 [04:16<10:01,  3.85s/it] gen pre-train\n",
            " 35% 85/240 [04:19<09:03,  3.51s/it] gen pre-train\n",
            " 36% 86/240 [04:22<08:24,  3.28s/it] gen pre-train\n",
            " 36% 87/240 [04:25<07:58,  3.13s/it] gen pre-train\n",
            " 37% 88/240 [04:27<07:37,  3.01s/it] gen pre-train\n",
            " 37% 89/240 [04:30<07:22,  2.93s/it] gen pre-train\n",
            " 38% 90/240 [04:33<07:09,  2.86s/it] gen pre-train\n",
            " 38% 91/240 [04:36<07:01,  2.83s/it] gen pre-train\n",
            " 38% 92/240 [04:38<06:53,  2.79s/it] gen pre-train\n",
            " 39% 93/240 [04:41<06:47,  2.77s/it] gen pre-train\n",
            " 39% 94/240 [04:44<06:41,  2.75s/it] gen pre-train\n",
            " 40% 95/240 [04:46<06:35,  2.73s/it] gen pre-train\n",
            " 40% 96/240 [04:49<06:33,  2.73s/it] gen pre-train\n",
            " 40% 97/240 [04:52<06:28,  2.72s/it] gen pre-train\n",
            " 41% 98/240 [04:55<06:26,  2.72s/it] gen pre-train\n",
            " 41% 99/240 [04:57<06:23,  2.72s/it] gen pre-train\n",
            " 42% 100/240 [05:00<06:19,  2.71s/it] gen pre-train\n",
            " 42% 101/240 [05:03<06:15,  2.70s/it] gen pre-train\n",
            " 42% 102/240 [05:05<06:14,  2.71s/it] gen pre-train\n",
            " 43% 103/240 [05:08<06:10,  2.71s/it] gen pre-train\n",
            " 43% 104/240 [05:11<06:07,  2.70s/it] gen pre-train\n",
            " 44% 105/240 [05:13<06:06,  2.72s/it] gen pre-train\n",
            " 44% 106/240 [05:16<06:02,  2.71s/it] gen pre-train\n",
            " 45% 107/240 [05:19<06:00,  2.71s/it] gen pre-train\n",
            " 45% 108/240 [05:22<05:57,  2.71s/it] gen pre-train\n",
            " 45% 109/240 [05:24<05:55,  2.71s/it] gen pre-train\n",
            " 46% 110/240 [05:27<05:53,  2.72s/it] gen pre-train\n",
            " 46% 111/240 [05:30<05:50,  2.72s/it] gen pre-train\n",
            " 47% 112/240 [05:32<05:48,  2.72s/it] gen pre-train\n",
            " 47% 113/240 [05:35<05:45,  2.72s/it] gen pre-train\n",
            " 48% 114/240 [05:38<05:43,  2.72s/it] gen pre-train\n",
            " 48% 115/240 [05:41<05:40,  2.72s/it] gen pre-train\n",
            " 48% 116/240 [05:43<05:38,  2.73s/it] gen pre-train\n",
            " 49% 117/240 [05:46<05:35,  2.73s/it] gen pre-train\n",
            " 49% 118/240 [05:49<05:32,  2.72s/it] gen pre-train\n",
            " 50% 119/240 [05:52<05:30,  2.73s/it] gen pre-train\n",
            " 50% 120/240 [05:54<05:28,  2.74s/it] gen pre-train\n",
            "\t test_loss 5.293943405151367, train_loss 0.720395565032959\n",
            "~~~ Summary Results ~~~\n",
            "Total samples   :   6400\n",
            "Unique          :   6397 (99.95%)\n",
            "Unverified      :   4479 (69.98%)\n",
            "Verified        :   1921 (30.02%)\n",
            "\tmetrics...\n",
            "novelty              : 1.0000\n",
            "hard_novelty         : 1.0000\n",
            "soft_novelty         : 1.0000\n",
            "diversity            : 0.4729\n",
            "conciseness          : 0.9804\n",
            "solubility           : 0.5208\n",
            "naturalness          : 0.5184\n",
            "synthesizability     : 0.5701\n",
            "\n",
            "Example of good samples:\n",
            "C/C(=C\\c1ccc(F)cc1)[C@H]1CCc2ccccc2C1\n",
            "O=C(CC1CCNCC1)Nc1ccc(OC(F)(F)F)cc1F\n",
            "O=C(Nc1ccc(F)c(F)c1)Nc1ccc(Br)cc1\n",
            "Cc1nn(C)c(C[C@H](O)c2ccc(OC)c(F)c2)c1\n",
            "Cc1nc(NC(=O)NC2([C@@H]3CCOC3)COC2)n(C)c1\n",
            "CCC[C@@H](S)c1ccc(F)cc1F\n",
            "COc1ccc(NCc2nccs2)cc1\n",
            "COc1ccc(CN(C)C(=O)c2ccc(Cl)cc2)cn1\n",
            "COc1ccc(C(=O)NCCc2cccs2)cn1\n",
            "CC(=O)[C@H](C)Oc1ccc2ccccc2c1C(=O)[NH2+]\n",
            "\n",
            "Example of bad samples:\n",
            "N(CCCCC)(O)C1(C)c2cccc(CS(=O)=O)c1)Oc1cc\n",
            "CCC1CCc2cc(S(=O)NNc3cc[nH]c3ccnc3)o2cccc\n",
            "C[NH2+](C)CCCCC(=O)NC1\n",
            "CCNC(=O)N1CCNC(=O)[C@H](C(=O)Oc2ccccc2)C\n",
            "CCC[C@H]1[NH+]CCC(NC)C[C@@H](C)CCCl)C1=O\n",
            "C[C@H](c1ccc(F)cc1)NCCNc1cc2ccc[nH+]2)s1\n",
            "CCC[NH2+]c1cn(CC(=O)NCCc2ccon2)n1\n",
            "C=CCOC(=O)[C@H](C)c1ccc(-c2ccon2)c1N\n",
            "CCc1noc(C2CCn2c3ccccc3c2N)c1\n",
            "C[NH+](C)C(=O)c1ccc(NC(=O)N(C)C(F)(F)F)c\n",
            "~~~~~~~~~~~~~~~~~~~~~~~\n",
            " 50% 121/240 [06:10<13:02,  6.57s/it] gen pre-train\n",
            " 51% 122/240 [06:13<10:40,  5.43s/it] gen pre-train\n",
            " 51% 123/240 [06:15<09:00,  4.62s/it] gen pre-train\n",
            " 52% 124/240 [06:18<07:51,  4.06s/it] gen pre-train\n",
            " 52% 125/240 [06:21<07:00,  3.66s/it] gen pre-train\n",
            " 52% 126/240 [06:24<06:26,  3.39s/it] gen pre-train\n",
            " 53% 127/240 [06:26<06:00,  3.19s/it] gen pre-train\n",
            " 53% 128/240 [06:29<05:41,  3.05s/it] gen pre-train\n",
            " 54% 129/240 [06:32<05:26,  2.95s/it] gen pre-train\n",
            " 54% 130/240 [06:34<05:16,  2.87s/it] gen pre-train\n",
            " 55% 131/240 [06:37<05:07,  2.82s/it] gen pre-train\n",
            " 55% 132/240 [06:40<05:00,  2.79s/it] gen pre-train\n",
            " 55% 133/240 [06:43<04:55,  2.76s/it] gen pre-train\n",
            " 56% 134/240 [06:45<04:51,  2.75s/it] gen pre-train\n",
            " 56% 135/240 [06:48<04:47,  2.73s/it] gen pre-train\n",
            " 57% 136/240 [06:51<04:43,  2.73s/it] gen pre-train\n",
            " 57% 137/240 [06:53<04:41,  2.74s/it] gen pre-train\n",
            " 57% 138/240 [06:56<04:37,  2.72s/it] gen pre-train\n",
            " 58% 139/240 [06:59<04:34,  2.72s/it] gen pre-train\n",
            " 58% 140/240 [07:02<04:31,  2.72s/it] gen pre-train\n",
            " 59% 141/240 [07:04<04:29,  2.72s/it] gen pre-train\n",
            " 59% 142/240 [07:07<04:25,  2.71s/it] gen pre-train\n",
            " 60% 143/240 [07:10<04:24,  2.72s/it] gen pre-train\n",
            " 60% 144/240 [07:12<04:21,  2.72s/it] gen pre-train\n",
            " 60% 145/240 [07:15<04:18,  2.72s/it] gen pre-train\n",
            " 61% 146/240 [07:18<04:15,  2.71s/it] gen pre-train\n",
            " 61% 147/240 [07:21<04:16,  2.75s/it] gen pre-train\n",
            " 62% 148/240 [07:23<04:12,  2.75s/it] gen pre-train\n",
            " 62% 149/240 [07:26<04:08,  2.73s/it] gen pre-train\n",
            " 62% 150/240 [07:29<04:05,  2.72s/it] gen pre-train\n",
            " 63% 151/240 [07:32<04:02,  2.73s/it] gen pre-train\n",
            " 63% 152/240 [07:34<03:58,  2.71s/it] gen pre-train\n",
            " 64% 153/240 [07:37<03:55,  2.71s/it] gen pre-train\n",
            " 64% 154/240 [07:40<03:53,  2.72s/it] gen pre-train\n",
            " 65% 155/240 [07:42<03:51,  2.73s/it] gen pre-train\n",
            " 65% 156/240 [07:45<03:48,  2.72s/it] gen pre-train\n",
            " 65% 157/240 [07:48<03:45,  2.72s/it] gen pre-train\n",
            " 66% 158/240 [07:51<03:42,  2.72s/it] gen pre-train\n",
            " 66% 159/240 [07:53<03:41,  2.74s/it] gen pre-train\n",
            " 67% 160/240 [07:56<03:40,  2.75s/it] gen pre-train\n",
            "\t test_loss 5.295933723449707, train_loss 0.7046123147010803\n",
            "~~~ Summary Results ~~~\n",
            "Total samples   :   6400\n",
            "Unique          :   6395 (99.92%)\n",
            "Unverified      :   4150 (64.84%)\n",
            "Verified        :   2250 (35.16%)\n",
            "\tmetrics...\n",
            "novelty              : 1.0000\n",
            "hard_novelty         : 1.0000\n",
            "soft_novelty         : 1.0000\n",
            "diversity            : 0.4502\n",
            "conciseness          : 0.9833\n",
            "solubility           : 0.5251\n",
            "naturalness          : 0.5068\n",
            "synthesizability     : 0.5841\n",
            "\n",
            "Example of good samples:\n",
            "Cc1c(Cl)csc1C(=O)Nc1ccc2c(c1C)N2c2ccccc2\n",
            "Cc1ccc(C[NH2+]Cc2ccccc2)cc1-n1cncn1\n",
            "Cc1ccc(C(=O)Nc2ccc(OC(=O)[O-])nc2)s1\n",
            "CCCCCCC(=O)Nc1nc(C)ccc1OC\n",
            "CCN(Cc1cnc(C(F)(F)F)cc1)CCNC(=O)C1COCC1\n",
            "CCCCNC(=O)C[C@H]1CC[NH2+]CC1\n",
            "CC(C)(Br)C1CC(O)c2ccccc21\n",
            "NC(=O)CN(c1ncccc1O)c1cccnc1\n",
            "CC(=O)[NH2+]CC(=O)Nc1ccsc1C\n",
            "COC1CC[C@@H](CO)c2ccccc21\n",
            "\n",
            "Example of bad samples:\n",
            "CC(C)CC(=O)CO[C@]1([C@H]2Cc2cccc3)c1\n",
            "Cc1cc[nH+]c(CCC(=O)Nc2cccc(Br)c2)o1\n",
            "CC(C)CC[C@H](c1ccc(F)cc1)C((C)C)C\n",
            "CCNC(=O)C[C@H](C)CN(Cc1ncccc12\n",
            "O=C(N[C@H]1CCC1)N1CCNC(=O)C(C)c1ccccc1)N\n",
            "CCc1n(C)nc1NC(=O)Nc1cnc(N[C@H]2CCC2)CC1)\n",
            "O=[S@]([NH+]CCC)C(=O)C1CCCC(C)C\n",
            "CC(C)Oc1ccc(C(C)(C(C)C)CCC2)c1F\n",
            "Fc1c(-n2cccnc23)ccc1NC(C)Cc1ccc(F)cc1\n",
            "COc1cccc(NCc2ccccc2C(F)(F)F)cc1\n",
            "~~~~~~~~~~~~~~~~~~~~~~~\n",
            " 67% 161/240 [08:13<09:23,  7.14s/it] gen pre-train\n",
            " 68% 162/240 [08:16<07:34,  5.82s/it] gen pre-train\n",
            " 68% 163/240 [08:19<06:17,  4.90s/it] gen pre-train\n",
            " 68% 164/240 [08:22<05:22,  4.24s/it] gen pre-train\n",
            " 69% 165/240 [08:24<04:44,  3.79s/it] gen pre-train\n",
            " 69% 166/240 [08:27<04:17,  3.47s/it] gen pre-train\n",
            " 70% 167/240 [08:30<03:56,  3.24s/it] gen pre-train\n",
            " 70% 168/240 [08:33<03:42,  3.08s/it] gen pre-train\n",
            " 70% 169/240 [08:35<03:31,  2.98s/it] gen pre-train\n",
            " 71% 170/240 [08:38<03:22,  2.89s/it] gen pre-train\n",
            " 71% 171/240 [08:41<03:16,  2.84s/it] gen pre-train\n",
            " 72% 172/240 [08:43<03:10,  2.81s/it] gen pre-train\n",
            " 72% 173/240 [08:46<03:06,  2.79s/it] gen pre-train\n",
            " 72% 174/240 [08:49<03:02,  2.76s/it] gen pre-train\n",
            " 73% 175/240 [08:52<02:58,  2.74s/it] gen pre-train\n",
            " 73% 176/240 [08:54<02:55,  2.75s/it] gen pre-train\n",
            " 74% 177/240 [08:57<02:52,  2.73s/it] gen pre-train\n",
            " 74% 178/240 [09:00<02:49,  2.74s/it] gen pre-train\n",
            " 75% 179/240 [09:03<02:47,  2.75s/it] gen pre-train\n",
            " 75% 180/240 [09:05<02:45,  2.76s/it] gen pre-train\n",
            " 75% 181/240 [09:08<02:42,  2.75s/it] gen pre-train\n",
            " 76% 182/240 [09:11<02:38,  2.74s/it] gen pre-train\n",
            " 76% 183/240 [09:13<02:35,  2.73s/it] gen pre-train\n",
            " 77% 184/240 [09:16<02:33,  2.74s/it] gen pre-train\n",
            " 77% 185/240 [09:19<02:30,  2.74s/it] gen pre-train\n",
            " 78% 186/240 [09:22<02:26,  2.72s/it] gen pre-train\n",
            " 78% 187/240 [09:24<02:24,  2.72s/it] gen pre-train\n",
            " 78% 188/240 [09:27<02:21,  2.72s/it] gen pre-train\n",
            " 79% 189/240 [09:30<02:18,  2.71s/it] gen pre-train\n",
            " 79% 190/240 [09:33<02:15,  2.71s/it] gen pre-train\n",
            " 80% 191/240 [09:35<02:12,  2.71s/it] gen pre-train\n",
            " 80% 192/240 [09:38<02:10,  2.72s/it] gen pre-train\n",
            " 80% 193/240 [09:41<02:08,  2.73s/it] gen pre-train\n",
            " 81% 194/240 [09:43<02:05,  2.73s/it] gen pre-train\n",
            " 81% 195/240 [09:46<02:02,  2.72s/it] gen pre-train\n",
            " 82% 196/240 [09:49<01:59,  2.73s/it] gen pre-train\n",
            " 82% 197/240 [09:52<01:56,  2.72s/it] gen pre-train\n",
            " 82% 198/240 [09:54<01:54,  2.72s/it] gen pre-train\n",
            " 83% 199/240 [09:57<01:51,  2.71s/it] gen pre-train\n",
            " 83% 200/240 [10:00<01:48,  2.72s/it] gen pre-train\n",
            "\t test_loss 5.292759895324707, train_loss 0.694700300693512\n",
            "~~~ Summary Results ~~~\n",
            "Total samples   :   6400\n",
            "Unique          :   6397 (99.95%)\n",
            "Unverified      :   4152 (64.88%)\n",
            "Verified        :   2248 (35.12%)\n",
            "\tmetrics...\n",
            "novelty              : 1.0000\n",
            "hard_novelty         : 1.0000\n",
            "soft_novelty         : 1.0000\n",
            "diversity            : 0.3987\n",
            "conciseness          : 0.9830\n",
            "solubility           : 0.5300\n",
            "naturalness          : 0.5123\n",
            "synthesizability     : 0.5635\n",
            "\n",
            "Example of good samples:\n",
            "CCC[C@H](OCC1CCc2ccccc2)CC1\n",
            "CC[NH2+]C[C@@H](CCOCOC)Cc1c(Cl)ccnn1\n",
            "O=C1C[NH+](C(=O)[C@@H]2CCSC2)CC1\n",
            "COC(C)(C)CNC(=O)[C@H](C)C[NH2+]C1CC1\n",
            "COc1so[n+]1C[NH+](C)Cc1ccccc1Cl\n",
            "[NH3+]Cc1ccc(N)c(C(=O)Oc2ccccc2OC)c1\n",
            "CC(C)N(Cc1ccc(C)cc1)c1nccs1\n",
            "CCCC[NH2+]c1ccccc1OC(=O)Nc1ccoc1\n",
            "CCC(F)(C)C[C@H](C)C(=O)N[C@]1(O)CCC1(O)C\n",
            "Cc1c[nH]c(C(=O)N2CCCC2)c1C\n",
            "\n",
            "Example of bad samples:\n",
            "CCC(=O)Nc1cnc(N2CCCN=CC2=OCO2)cc1oc1cccc\n",
            "Cc1cc(F)c(C(=O)NOc2cccc(Cl)c2F)n1\n",
            "O=C(CC1(C)CCc2ccccc2C1=O\n",
            "CCOc1ccc(N2CCN(C(O)=N3)CC2=O)cc1\n",
            "Cc1csc2nccc2n1CCN1\n",
            "CCCCC[NH3+])=O\n",
            "Cs1CCc2cccc(NC(=O)[C@H]3c3cc(F)ccc3)c(-c\n",
            "O=C(/C(c3cccc3)no1\n",
            "O=C(Oc1cccc(O)c1)Nc1cc[n2-]\n",
            "COc1ccc2c(c1)OCN(CCSC)c2ccccc21\n",
            "~~~~~~~~~~~~~~~~~~~~~~~\n",
            " 84% 201/240 [10:17<04:32,  7.00s/it] gen pre-train\n",
            " 84% 202/240 [10:19<03:37,  5.71s/it] gen pre-train\n",
            " 85% 203/240 [10:22<02:57,  4.81s/it] gen pre-train\n",
            " 85% 204/240 [10:25<02:30,  4.18s/it] gen pre-train\n",
            " 85% 205/240 [10:28<02:10,  3.74s/it] gen pre-train\n",
            " 86% 206/240 [10:30<01:56,  3.43s/it] gen pre-train\n",
            " 86% 207/240 [10:33<01:45,  3.21s/it] gen pre-train\n",
            " 87% 208/240 [10:36<01:37,  3.06s/it] gen pre-train\n",
            " 87% 209/240 [10:38<01:31,  2.96s/it] gen pre-train\n",
            " 88% 210/240 [10:41<01:26,  2.89s/it] gen pre-train\n",
            " 88% 211/240 [10:44<01:22,  2.84s/it] gen pre-train\n",
            " 88% 212/240 [10:47<01:18,  2.81s/it] gen pre-train\n",
            " 89% 213/240 [10:49<01:15,  2.78s/it] gen pre-train\n",
            " 89% 214/240 [10:52<01:12,  2.79s/it] gen pre-train\n",
            " 90% 215/240 [10:55<01:09,  2.78s/it] gen pre-train\n",
            " 90% 216/240 [10:58<01:06,  2.76s/it] gen pre-train\n",
            " 90% 217/240 [11:00<01:03,  2.75s/it] gen pre-train\n",
            " 91% 218/240 [11:03<01:00,  2.73s/it] gen pre-train\n",
            " 91% 219/240 [11:06<00:57,  2.72s/it] gen pre-train\n",
            " 92% 220/240 [11:08<00:54,  2.72s/it] gen pre-train\n",
            " 92% 221/240 [11:11<00:51,  2.73s/it] gen pre-train\n",
            " 92% 222/240 [11:14<00:48,  2.72s/it] gen pre-train\n",
            " 93% 223/240 [11:17<00:46,  2.74s/it] gen pre-train\n",
            " 93% 224/240 [11:19<00:43,  2.73s/it] gen pre-train\n",
            " 94% 225/240 [11:22<00:40,  2.72s/it] gen pre-train\n",
            " 94% 226/240 [11:25<00:38,  2.71s/it] gen pre-train\n",
            " 95% 227/240 [11:27<00:35,  2.72s/it] gen pre-train\n",
            " 95% 228/240 [11:30<00:32,  2.70s/it] gen pre-train\n",
            " 95% 229/240 [11:33<00:29,  2.71s/it] gen pre-train\n",
            " 96% 230/240 [11:36<00:27,  2.71s/it] gen pre-train\n",
            " 96% 231/240 [11:38<00:24,  2.71s/it] gen pre-train\n",
            " 97% 232/240 [11:41<00:21,  2.71s/it] gen pre-train\n",
            " 97% 233/240 [11:44<00:18,  2.71s/it] gen pre-train\n",
            " 98% 234/240 [11:46<00:16,  2.70s/it] gen pre-train\n",
            " 98% 235/240 [11:49<00:13,  2.70s/it] gen pre-train\n",
            " 98% 236/240 [11:52<00:10,  2.70s/it] gen pre-train\n",
            " 99% 237/240 [11:55<00:08,  2.72s/it] gen pre-train\n",
            " 99% 238/240 [11:57<00:05,  2.72s/it] gen pre-train\n",
            "100% 239/240 [12:00<00:02,  2.72s/it] gen pre-train\n",
            "100% 240/240 [12:03<00:00,  3.01s/it]\n",
            "Start training discriminator...\n",
            "  0% 0/50 [00:00<?, ?it/s] discriminator pre-train\n",
            "/content/ChemGAN-challenge/model/dis_dataloader.py:74: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  data = np.array(list(data))\n",
            "\tD loss  :   3.645660400390625\n",
            "\tAccuracy: 0.5\n",
            "  2% 1/50 [06:09<5:01:50, 369.59s/it] discriminator pre-train\n",
            "\tD loss  :   2.621131181716919\n",
            "\tAccuracy: 0.625\n",
            "  4% 2/50 [12:23<4:56:40, 370.84s/it] discriminator pre-train\n",
            "\tD loss  :   2.2353568077087402\n",
            "\tAccuracy: 0.6875\n",
            "  6% 3/50 [18:41<4:52:16, 373.13s/it] discriminator pre-train\n",
            "\tD loss  :   1.914555311203003\n",
            "\tAccuracy: 0.6875\n",
            "  8% 4/50 [25:01<4:47:31, 375.02s/it] discriminator pre-train\n",
            "\tD loss  :   1.4957735538482666\n",
            "\tAccuracy: 0.6875\n",
            " 10% 5/50 [31:22<4:42:40, 376.89s/it] discriminator pre-train\n",
            "\tD loss  :   1.392386794090271\n",
            "\tAccuracy: 0.625\n",
            " 12% 6/50 [37:43<4:37:18, 378.15s/it] discriminator pre-train\n",
            "\tD loss  :   1.1556329727172852\n",
            "\tAccuracy: 0.875\n",
            " 14% 7/50 [44:05<4:31:42, 379.13s/it] discriminator pre-train\n",
            "\tD loss  :   0.9127695560455322\n",
            "\tAccuracy: 0.8125\n",
            " 16% 8/50 [50:28<4:26:14, 380.35s/it] discriminator pre-train\n",
            "\tD loss  :   0.8958619832992554\n",
            "\tAccuracy: 0.75\n",
            " 18% 9/50 [56:48<4:19:57, 380.42s/it] discriminator pre-train\n",
            "\tD loss  :   0.5254783630371094\n",
            "\tAccuracy: 0.9375\n",
            " 20% 10/50 [1:03:10<4:13:49, 380.75s/it] discriminator pre-train\n",
            "\tD loss  :   0.5106770992279053\n",
            "\tAccuracy: 0.9375\n",
            " 22% 11/50 [1:09:32<4:07:51, 381.33s/it] discriminator pre-train\n",
            "\tD loss  :   0.3392437994480133\n",
            "\tAccuracy: 1.0\n",
            " 24% 12/50 [1:15:55<4:01:46, 381.75s/it] discriminator pre-train\n",
            "\tD loss  :   0.2753782272338867\n",
            "\tAccuracy: 1.0\n",
            " 26% 13/50 [1:22:17<3:55:25, 381.77s/it] discriminator pre-train\n",
            "\tD loss  :   0.2853093147277832\n",
            "\tAccuracy: 1.0\n",
            " 28% 14/50 [1:28:38<3:48:53, 381.50s/it] discriminator pre-train\n",
            "\tD loss  :   0.3465290069580078\n",
            "\tAccuracy: 0.9375\n",
            " 30% 15/50 [1:35:00<3:42:36, 381.61s/it] discriminator pre-train\n",
            "\tD loss  :   0.2356654703617096\n",
            "\tAccuracy: 1.0\n",
            " 32% 16/50 [1:41:22<3:36:22, 381.82s/it] discriminator pre-train\n",
            "\tD loss  :   0.203791081905365\n",
            "\tAccuracy: 1.0\n",
            " 34% 17/50 [1:47:45<3:30:07, 382.03s/it] discriminator pre-train\n",
            "\tD loss  :   0.11351696401834488\n",
            "\tAccuracy: 1.0\n",
            " 36% 18/50 [1:54:08<3:23:57, 382.42s/it] discriminator pre-train\n",
            "\tD loss  :   0.1390770673751831\n",
            "\tAccuracy: 1.0\n",
            " 38% 19/50 [2:00:30<3:17:35, 382.43s/it] discriminator pre-train\n",
            "\tD loss  :   0.09614257514476776\n",
            "\tAccuracy: 1.0\n",
            " 40% 20/50 [2:06:51<3:10:57, 381.92s/it] discriminator pre-train\n",
            "\tD loss  :   0.15831609070301056\n",
            "\tAccuracy: 0.9375\n",
            " 42% 21/50 [2:13:13<3:04:38, 382.03s/it] discriminator pre-train\n",
            "\tD loss  :   0.07463439553976059\n",
            "\tAccuracy: 1.0\n",
            " 44% 22/50 [2:19:36<2:58:20, 382.17s/it] discriminator pre-train\n",
            "\tD loss  :   0.11821388453245163\n",
            "\tAccuracy: 1.0\n",
            " 46% 23/50 [2:25:59<2:52:07, 382.49s/it] discriminator pre-train\n",
            "\tD loss  :   0.04623882472515106\n",
            "\tAccuracy: 1.0\n",
            " 48% 24/50 [2:32:22<2:45:46, 382.55s/it] discriminator pre-train\n",
            "\tD loss  :   0.06539861857891083\n",
            "\tAccuracy: 1.0\n",
            " 50% 25/50 [2:38:43<2:39:10, 382.04s/it] discriminator pre-train\n",
            "\tD loss  :   0.05003800988197327\n",
            "\tAccuracy: 1.0\n",
            " 52% 26/50 [2:45:06<2:32:55, 382.31s/it] discriminator pre-train\n",
            "\tD loss  :   0.03740978240966797\n",
            "\tAccuracy: 1.0\n",
            " 54% 27/50 [2:51:27<2:26:25, 381.99s/it] discriminator pre-train\n",
            "\tD loss  :   0.031476251780986786\n",
            "\tAccuracy: 1.0\n",
            " 56% 28/50 [2:57:48<2:19:58, 381.73s/it] discriminator pre-train\n",
            "\tD loss  :   0.06888598948717117\n",
            "\tAccuracy: 1.0\n",
            " 58% 29/50 [3:04:12<2:13:47, 382.27s/it] discriminator pre-train\n",
            "\tD loss  :   0.04717952013015747\n",
            "\tAccuracy: 1.0\n",
            " 60% 30/50 [3:10:34<2:07:27, 382.38s/it] discriminator pre-train\n",
            "\tD loss  :   0.056056007742881775\n",
            "\tAccuracy: 1.0\n",
            " 62% 31/50 [3:16:58<2:01:12, 382.75s/it] discriminator pre-train\n",
            "\tD loss  :   0.04964275658130646\n",
            "\tAccuracy: 1.0\n",
            " 64% 32/50 [3:23:20<1:54:45, 382.52s/it] discriminator pre-train\n",
            "\tD loss  :   0.023318806663155556\n",
            "\tAccuracy: 1.0\n",
            " 66% 33/50 [3:29:42<1:48:20, 382.36s/it] discriminator pre-train\n",
            "\tD loss  :   0.06724497675895691\n",
            "\tAccuracy: 1.0\n",
            " 68% 34/50 [3:36:04<1:41:58, 382.42s/it] discriminator pre-train\n",
            "\tD loss  :   0.07576628029346466\n",
            "\tAccuracy: 1.0\n",
            " 70% 35/50 [3:42:27<1:35:38, 382.54s/it] discriminator pre-train\n",
            "\tD loss  :   0.034569982439279556\n",
            "\tAccuracy: 1.0\n",
            " 72% 36/50 [3:48:50<1:29:17, 382.65s/it] discriminator pre-train\n",
            "\tD loss  :   0.04128389060497284\n",
            "\tAccuracy: 1.0\n",
            " 74% 37/50 [3:55:13<1:22:54, 382.66s/it] discriminator pre-train\n",
            "\tD loss  :   0.0188416950404644\n",
            "\tAccuracy: 1.0\n",
            " 76% 38/50 [4:01:37<1:16:36, 383.05s/it] discriminator pre-train\n",
            "\tD loss  :   0.024156562983989716\n",
            "\tAccuracy: 1.0\n",
            " 78% 39/50 [4:07:59<1:10:12, 382.95s/it] discriminator pre-train\n",
            "\tD loss  :   0.043849680572748184\n",
            "\tAccuracy: 1.0\n",
            " 80% 40/50 [4:14:25<1:03:57, 383.75s/it] discriminator pre-train\n",
            "\tD loss  :   0.014193573966622353\n",
            "\tAccuracy: 1.0\n",
            " 82% 41/50 [4:20:49<57:34, 383.83s/it]   discriminator pre-train\n",
            "\tD loss  :   0.013458766974508762\n",
            "\tAccuracy: 1.0\n",
            " 84% 42/50 [4:27:13<51:10, 383.79s/it] discriminator pre-train\n",
            "\tD loss  :   0.013164648786187172\n",
            "\tAccuracy: 1.0\n",
            " 86% 43/50 [4:33:35<44:44, 383.49s/it] discriminator pre-train\n",
            "\tD loss  :   0.01702895388007164\n",
            "\tAccuracy: 1.0\n",
            " 88% 44/50 [4:40:00<38:22, 383.80s/it] discriminator pre-train\n",
            "\tD loss  :   0.024247046560049057\n",
            "\tAccuracy: 1.0\n",
            " 90% 45/50 [4:46:25<32:00, 384.05s/it] discriminator pre-train\n",
            "\tD loss  :   0.011099323630332947\n",
            "\tAccuracy: 1.0\n",
            " 92% 46/50 [4:52:49<25:36, 384.09s/it] discriminator pre-train\n",
            "\tD loss  :   0.019561953842639923\n",
            "\tAccuracy: 1.0\n",
            " 94% 47/50 [4:59:13<19:12, 384.23s/it] discriminator pre-train\n",
            "\tD loss  :   0.0097582396119833\n",
            "\tAccuracy: 1.0\n",
            " 96% 48/50 [5:05:38<12:48, 384.48s/it] discriminator pre-train\n",
            "\tD loss  :   0.021808136254549026\n",
            "\tAccuracy: 1.0\n",
            " 98% 49/50 [5:12:02<06:24, 384.24s/it] discriminator pre-train\n",
            "\tD loss  :   0.010515691712498665\n",
            "\tAccuracy: 1.0\n",
            "100% 50/50 [5:18:25<00:00, 382.10s/it]\n",
            "Total time was 19833.2383s\n",
            "Pretrain finished and saved at checkpoints/textCNN_pretrain/pretrain_ckpt\n",
            "#########################################################################\n",
            "Start Reinforcement Training Generator...\n",
            "  0% 0/2 [00:00<?, ?it/s]* Making samples\n",
            "batch_num: 0\n",
            "test_loss: 5.283829212188721\n",
            "best score: 5.283829\n",
            "~~~ Summary Results ~~~\n",
            "Total samples   :  32000\n",
            "Unique          :  31963 (99.88%)\n",
            "Unverified      :  20080 (62.75%)\n",
            "Verified        :  11920 (37.25%)\n",
            "\tmetrics...\n",
            "novelty              : 0.9998\n",
            "hard_novelty         : 0.9998\n",
            "soft_novelty         : 0.9999\n",
            "diversity            : 0.4260\n",
            "conciseness          : 0.9835\n",
            "solubility           : 0.5412\n",
            "naturalness          : 0.5064\n",
            "synthesizability     : 0.5817\n",
            "\n",
            "Example of good samples:\n",
            "CC(F)(CC)NC(=O)N1CCCO[C@@H]1C\n",
            "COCCN(CC#N)CCOc1ccccc1\n",
            "CCNC(=O)c1cc[nH+]cc1N1CCOC1\n",
            "OC1(Cc2ccccc2)nc1-n1nccc1C\n",
            "O=C([O-])c1ccc(O)o1\n",
            "COC(=O)c1ccc(C(=O)N2CC(C)C2)cn1\n",
            "CCN1CCC(OC(=O)c2ccc(F)cc2)CC1\n",
            "COC[C@H](NC(=O)Nc1ccccn1)C1CC1\n",
            "CC(=O)Nc1ccc(NC(=O)CCNc2ccccc2)[nH]1\n",
            "CO[C@@H]([NH+]1CCCCC1)c1ccccc1\n",
            "\n",
            "Example of bad samples:\n",
            "CCOc1cc(C=C(=O)N(C)C)c(C)c(=O)s1\n",
            "Cc1sc(-c2ccnc(C)c2=O)c(C)n1\n",
            "O=C1CCO(C(C)(C)C2)C1\n",
            "OCc1csc(C3/C=C/c3cccc3ccccc23)c1\n",
            "CC(C)=N1CC[C@H](n2c3ccc2cncn3C)cn1\n",
            "CCc1nnn[nH]1Cc2ccccc2C1Cn1c(O)ccc1CO\n",
            "CN(Cc1cccs1)C(=O)c1ccc(OCCN(C)C)n1\n",
            "C[C@H]1CC[C@H]1OCc1ccc2c(c1)C(C)C\n",
            "CC1(C)NC(=O)N1CCSCC1\n",
            "CCOc1cnc(Cn2nccc2C[C@@H]2CCC3)C[C@@H]1OC\n",
            "~~~~~~~~~~~~~~~~~~~~~~~\n",
            "#########################################################################\n",
            "-> Training generator with RL.\n",
            "G Epoch 0\n",
            "Rewards be like...\n",
            "[[0.153 0.221 0.158 ... 0.155 0.155 0.155]\n",
            " [0.185 0.071 0.09  ... 0.255 0.255 0.255]\n",
            " [0.061 0.077 0.151 ... 0.021 0.021 0.021]\n",
            " ...\n",
            " [0.158 0.08  0.153 ... 0.01  0.01  0.01 ]\n",
            " [0.142 0.159 0.159 ... 0.322 0.322 0.322]\n",
            " [0.131 0.218 0.268 ... 0.    0.    0.   ]]\n",
            "Mean: 0.113 , Std:  0.110, Min: 0.000 , Max:  0.687\n",
            "\n",
            "neg-loglike: 184.25289916992188\n",
            "-> Training Discriminator\n",
            "D_Epoch 0\n",
            "\tD loss  :   0.031210709363222122\n",
            "\tAccuracy: 1.0\n",
            "D_Epoch 1\n",
            "\tD loss  :   0.03847759589552879\n",
            "\tAccuracy: 1.0\n",
            "results\n",
            "Model saved at checkpoints/textCNN/textCNN_model.ckpt\n",
            " 50% 1/2 [17:08<17:08, 1028.76s/it]* Making samples\n",
            "batch_num: 1\n",
            "test_loss: 5.287023544311523\n",
            "~~~ Summary Results ~~~\n",
            "Total samples   :   6400\n",
            "Unique          :   6392 (99.88%)\n",
            "Unverified      :   3555 (55.55%)\n",
            "Verified        :   2845 (44.45%)\n",
            "\tmetrics...\n",
            "novelty              : 1.0000\n",
            "hard_novelty         : 1.0000\n",
            "soft_novelty         : 1.0000\n",
            "diversity            : 0.3986\n",
            "conciseness          : 0.9860\n",
            "solubility           : 0.5534\n",
            "naturalness          : 0.5015\n",
            "synthesizability     : 0.6221\n",
            "\n",
            "Example of good samples:\n",
            "Cc1cc(C)ccc1OC(=O)c1ccccc1C(F)F\n",
            "N#CCCCc1scc(SC)c1Br\n",
            "CNC(=O)[C@@H]([NH3+])c1ccccc1C#N\n",
            "S=C(NCCS(C)C)C(=O)c1ccc(Cl)s1\n",
            "O=C(NCCN1CCOC1=O)N1CCOO1\n",
            "Cc1ccc(CC(=O)NC2C[C@H](CO)C2)cc1\n",
            "O=S(=O)(NC(=O)CC#N)CCC#N\n",
            "Cc1nocc1CC(=O)N1CCN(C(N)=O)CCCC1\n",
            "COC(=O)N[C@@H](Cc1cccs1)c1ccccc1O\n",
            "C[C@H]([NH3+])C(=O)C(C)C\n",
            "\n",
            "Example of bad samples:\n",
            "Oc1ccc(NC(=O)NC(=O)CCSc3ccccc3)Nc2nccc(C\n",
            "CC(C)=OC(C)[C@@H](C)OC\n",
            "CC(C)Oc1ccccc1[C@H]1CC(=O)[C@@H]1CCOc1cc\n",
            "[O-]c1ccc2c(c2ccccc3o2)c1\n",
            "C=CCOC(=O)c1cscc1CN1Nc1ccc[nH+]1\n",
            "COc1ccc(C(=O)NCC/S(C)(=O)=O)[C@H](Cc2ccc\n",
            "Cc1cccc(N(C)C(=O)Cc3cc(C)csc2=O)N1CCc1cc\n",
            "CCc1ccc(-c2nnc(CN3CCOCC4CC3)s2)c[nH]1\n",
            "CCS(=O)(=O)CCNC(=O)COc1cccn1\n",
            "CCc1nc2ccccc2n1CCCCCC1\n",
            "~~~~~~~~~~~~~~~~~~~~~~~\n",
            "#########################################################################\n",
            "-> Training generator with RL.\n",
            "G Epoch 1\n",
            "Rewards be like...\n",
            "[[0.199 0.244 0.216 ... 0.    0.    0.   ]\n",
            " [0.107 0.166 0.203 ... 0.    0.    0.   ]\n",
            " [0.129 0.123 0.132 ... 0.175 0.175 0.175]\n",
            " ...\n",
            " [0.124 0.104 0.079 ... 0.313 0.313 0.313]\n",
            " [0.132 0.127 0.071 ... 0.176 0.176 0.176]\n",
            " [0.059 0.142 0.222 ... 0.    0.    0.   ]]\n",
            "Mean: 0.125 , Std:  0.111, Min: 0.000 , Max:  0.835\n",
            "\n",
            "neg-loglike: 193.4894561767578\n",
            "-> Training Discriminator\n",
            "D_Epoch 0\n",
            "\tD loss  :   0.009263685904443264\n",
            "\tAccuracy: 1.0\n",
            "D_Epoch 1\n",
            "\tD loss  :   0.007496862672269344\n",
            "\tAccuracy: 1.0\n",
            "results\n",
            "100% 2/2 [33:20<00:00, 1000.32s/it]\n",
            "Model saved at checkpoints/textCNN/textCNN_model.ckpt\n",
            "\n",
            ":*** FINISHED ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xSF-6n-Ms17",
        "outputId": "3f5493c6-d3f4-41ab-8cca-5cedfa09567f"
      },
      "source": [
        "!curl -L bit.ly/rdkit-colab | tar xz -C /"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   163  100   163    0     0    783      0 --:--:-- --:--:-- --:--:--   783\n",
            "100   638  100   638    0     0   1263      0 --:--:-- --:--:-- --:--:--  1263\n",
            "100 26.3M  100 26.3M    0     0  12.2M      0  0:00:02  0:00:02 --:--:-- 16.6M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xRcVlLWVzJt",
        "outputId": "e26d23bc-0367-4c03-e166-2fbbf7190907"
      },
      "source": [
        "ls\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}